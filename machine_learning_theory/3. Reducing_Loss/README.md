# Reducing Loss : An Iterative Approach

머신러닝이 반복을 통해 어떻게 손실을 줄이는지 알아 보겠습니다.  
반복학습은 숨겨진 물건을 찾는 [Hot and Cold](https://www.howcast.com/videos/258352-how-to-play-hot-and-cold) 놀이와 비슷 합니다. 이 놀이에서 **숨겨진 물건**이 최적 모델이 됩니다. 처음 임의의 지점에서 시작해서(w1의 값은 0) 시스템이 손실 값을 알려줄때까지 기다립니다. 그런 다음 다른 값을 추정해서(w1의 값은 0.5) 손실 값을 확인 합니다. 사실 이 방식은 제대로만 하면 점점 가까워지게 되어 있습니다. 진짜 중요한 것은 최적의 모델을 가능한 효율적으로 찾는 것 입니다.  
다음은 머신러닝 알고리즘이 모델을 학습하는데 사용하는 반복적인 시행착오 과정을 보여줍니다.

<p align="center">
<img src="https://user-images.githubusercontent.com/48028667/93763634-c1f51580-fc4c-11ea-807e-403176316d71.PNG" style="width: 600px">
</p>

<br />

반복 전략은 주로 대규모 데이터 세트에 적용하기 용이하여 머신러닝 전반에서 널리 사용되고 있습니다. 이 모델은 하나 이상의 특성(feature)을 입력하여 하나의 예측(y')을 출력합니다.  
이해를 위해 하나의 특성을 가지고 하나의 예측을 반환하는 모델을 생각해 보겠습니다.

<br />

<p align="center">
y' = b + w1x1
</p>

<br />

b와 w1의 초기값을 무엇으로 설정해야 할까요?  
선형 회귀 문제에서 초기값은 별로 중요하지 않습니다. 임의의 값을 정해도 되지만 일단 다음 값을 사용해 보겠습니다.

- b = 0
- w1 = 0

최초 특성 값이 10이라고 가정하겠습니다. 이 특성 값을 예측 함수(prediction function)에 입력하면 다음과 같이 출력 됩니다.

- y' = 0 + 0(10)
- y' = 0

위의 다이어 그램에서 '손실 계산' 과정은 이 모델에서 사용한 [손실 함수](https://github.com/JongMinLee0/nlp_study/tree/master/machine_learning_theory/Descending_into_ml) 입니다. 손실 함수는 두개의 입력 값을 취합니다.

- y' : 특성 x에 대한 모델의 예측 값 입니다.
- y : 특성 x에 대한 올바른 라벨 입니다.

다이어그램은 이제 '매개변수 업데이트 계산' 과정에 도달합니다. 이 지점에서 머신러닝 시스템은 손실 함수의 값을 검토하여 **b**와 **w1**의 새로운 값을 생성 합니다. 이 값을 이용하여 다시 값을 출력하고 손실값이 가장 낮은 모델 매개 변수를 발견할 때까지 반복 학습합니다. 전체 손실이 변하지 않거나 매우 느리게 변할때 모델이 **수렴(converged)** 했다고 합니다.


# Reducing Loss : Gredient Desent

반복 방식(An Iterative Approach)에서는 **매개변수 업데이트 계산**이라는 다이어그램이 포함되어 있었습니다. 이 애매모호한 알고리즘을 좀 더 실질적인 것으로 대체해 보겠습니다.  
w1의 가능한 모든 값에 대해 손실을 계산할 시간과 컴퓨팅 자료가 있다고 가정합니다. 지금까지 살펴본 것과 같은 회귀 문제에서 손실과 w1을 대응한 도표는 항상 볼록 함수 모양을 할 것 입니다. 즉 도표가 다음과 같이 항상 그릇 모양으로 나타납니다.  

<p align="center">
<img src="https://user-images.githubusercontent.com/48028667/94564006-6bb65100-02a2-11eb-9d35-82310cf4d238.PNG" style="width: 400px">
</p>

### **회귀 문제에서는 볼록 함수 모양의 손실대 가중치 도표가 산출됩니다.**  
볼록 문제에서는 기울기가 정확하게 0인 지점이 하나만 존재합니다. 이 최소값에 손실 함수가 수렴합니다.  
전체 데이터 세트에 대해 상상할 수 있는 모든 w1값의 손실 함수를 계산하는 것은 비효율적인 방법 입니다.  
머신러닝에서 널리 사용하는 방법인 **경사하강법**을 살펴 보겠습니다.  

<br />

경사하강법의 첫 번째 단계는 w1에 대한 시작값(시작점)을 선택하는 것 입니다. 시작점은 별로 중요하지 않습니다. 따라서 많은 알고리즘에서는 w1을 0으로 설정하거나 임의의 값을 선택합니다. 다음의 그림에서는 0보다 조금 큰 시작점을 지정했습니다.

<p align="center">
<img src="https://user-images.githubusercontent.com/48028667/94564012-6e18ab00-02a2-11eb-8d54-229a90f962c7.PNG" style="width: 400px">
</p>

### **경사하강법의 시작점**  
경사하강법 알고리즘은 시작점에서 손실 곡선의 기울기를 계산합니다. 간단히 말하자면 기울기는 편미분의 벡터로서, 어느 방향이 '더 정확한지' 혹은 '더 부정확한지' 알려줍니다.  
기울기는 벡터이므로 다음 두 가지 특성을 모두 가지고 있습니다.  
- 방향
- 크기

기울기는 항상 손실 함수 값이 가장 크게 증가하는 방향으로 향합니다. 경사하강법 알고리즘은 가능한 한 빨리 손실을 줄이기 위해 기울기의 반대 방향으로 이동합니다.

<p align="center">
<img src="https://user-images.githubusercontent.com/48028667/94564024-707b0500-02a2-11eb-81ee-ca4673c80f4c.PNG" style="width: 400px">
</p>


### **경사하강법은 음의 기울기를 사용합니다.**  
손실 함수 곡선의 다음 지점을 결정하기 위해 경사하강법 알고리즘은 다음과 같이 기울기의 크기 일부를 시작점에서 더합니다.
<p align="center">
<img src="https://user-images.githubusercontent.com/48028667/94564027-72dd5f00-02a2-11eb-9be8-23e93d2501cc.PNG" style="width: 400px">
</p>  

### **기울기 보폭을 통해 손실 곡선의 다음 지점으로 이동합니다.**  
그런 다음 경사하강법은 이 과정을 반복해 최소값에 점점 접근합니다.  


<br />


# Reducing Loss : Learning Rate

위에서 살펴본 경사하강법 알고리즘은 기울기에 **학습률**(learning rate) 혹은 **step size**라는 스칼라 값을 곱하여 다음 지점을 결정 합니다.  
예를 들어 기울기가 2.5이고 학습률이 0.01이면 경사하강법 알고리즘은 이전 지점으로부터 0.025 떨어진 지점을 다음 지점으로 결정합니다.   
초매개변수(Hyperparameters)는 프로그래머가 아닌 머신러닝 알고리즘에서 조정하는 값입니다. 대부분의 머신러닝 프로그래머는 학습률을 미세조정하는데 상당한 시간을 소비 합니다. 학습률을 너무 작게 설정하면 학습 시간이 매우 오래 걸릴 것 입니다.
<p align="center">
<img src="https://user-images.githubusercontent.com/48028667/94986337-fea00580-0598-11eb-83f6-adbaed4dbfb8.PNG" style="width: 400px">
</p>  

<br />

반대로 학습률을 너무 크게 설정하면 양자역학 실험을 잘못한 것 처럼 다음 지점이 곡선의 최저점에서 무질서하게 이탈할 우려가 있습니다.
<p align="center">
<img src="https://user-images.githubusercontent.com/48028667/94986339-0069c900-0599-11eb-9f20-5ee7962a64cd.PNG" style="width: 400px">
</p>  

<br />

모든 회귀 문제에서는 [Goldilocks](https://en.wikipedia.org/wiki/Goldilocks_principle) 학습률이 있습니다. 골리락스 값은 손실함수가 얼마나 평탄한지 여부와 관련이 있습니다. 손실함수의 기울기가 작다면 더 큰 학습률을 시도해 볼 수 있습니다. 이렇게 하면 작은 기울기를 보완하고 더 큰 보폭(step size)를 만들어 낼 수 있습니다.
<p align="center">
<img src="https://user-images.githubusercontent.com/48028667/94986340-02cc2300-0599-11eb-8973-79c665a1ab39.PNG" style="width: 400px">
</p>  

<br />


